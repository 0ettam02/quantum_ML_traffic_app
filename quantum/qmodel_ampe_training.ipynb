{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5325104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoche = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee30bfec",
   "metadata": {},
   "source": [
    "# Algoritmo originale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "from pennylane import numpy as np\n",
    "import pennylane as qml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Parse the script arguments.\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--dataset-path\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the dataset.\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the output directory.\"\n",
    "    )\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def ingest_dataset(path, n_pkts=10, n_features=4):\n",
    "    with open(path, \"rb\") as f:\n",
    "        biflows = pickle.load(f)\n",
    "        labels = pickle.load(f)\n",
    "    biflows = np.array(biflows)[:,:n_pkts,:n_features]\n",
    "    return biflows, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parsing degli argomenti\n",
    "    args = get_args()\n",
    "    dataset_path = args.dataset_path\n",
    "    output_dir = args.output_dir\n",
    "\n",
    "    n_pkts = 10\n",
    "    n_features = 4\n",
    "    seed = 2025\n",
    "    \n",
    "    n_qubits = 5\n",
    "    n_layers = 3\n",
    "\n",
    "    # Riproducibilità\n",
    "    np.random.seed(seed)                    # NumPy\n",
    "    tf.random.set_seed(seed)                # TensorFlow\n",
    "    tf.keras.utils.set_random_seed(seed)    # Keras\n",
    "\n",
    "    # Caricamento del dataset\n",
    "    X, y = ingest_dataset(dataset_path, n_pkts=n_pkts, n_features=n_features)\n",
    "    num_classes = len(np.unique(y))\n",
    "\n",
    "    # Codifica delle label\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    # Partizionamento in train, validation e test set (proporzioni 80/20, 80/20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed, stratify=y,\n",
    "    )\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(np.reshape(X_train, [-1, n_features]))\n",
    "    res_samples_train = scaler.transform(np.reshape(X_train, [-1, n_features]))\n",
    "    res_samples_test = scaler.transform(np.reshape(X_test, [-1, n_features]))\n",
    "    X_train = np.reshape(res_samples_train, [-1, n_pkts, n_features])\n",
    "    X_test = np.reshape(res_samples_test, [-1, n_pkts, n_features])\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=seed, stratify=y_train,\n",
    "    )\n",
    "\n",
    "    ohe_y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "    ohe_y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=num_classes)\n",
    "\n",
    "    # Definizione del circuito quantistico\n",
    "    dev = qml.device(\"default.qubit\", seed=seed, wires=n_qubits)\n",
    "    @qml.qnode(dev , interface=\"tf\")\n",
    "    def qnode(inputs, weights):\n",
    "        \n",
    "        # Feature map\n",
    "        qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True, pad_with=0.0)\n",
    "\n",
    "        # Ansatz\n",
    "        qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "\n",
    "        # Processo di misurazione\n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    # Pesi dell'ansatz\n",
    "    weights = np.random.rand(n_layers*n_qubits*3).reshape(n_layers,n_qubits,3)\n",
    "    weights = tf.Variable(weights, dtype=tf.float64, trainable=True)\n",
    "    weight_shapes = {\"weights\": (n_layers,n_qubits,3)}\n",
    "\n",
    "    #Definizione del modello\n",
    "    model = Sequential(name='ampe_fixed_probs')\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2**n_qubits,activation='sigmoid', input_dim=n_pkts*n_features))\n",
    "    model.add(qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=2**n_qubits)) # <-- qui c'è la parte quantum\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.build(np.shape(X_train))\n",
    "\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "    callbacks = [earlystop]\n",
    "\n",
    "    os.makedirs(f\"{output_dir}\", exist_ok=True)\n",
    "\n",
    "    with open(f\"{output_dir}/model_summary.txt\", \"w\") as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "    history = model.fit(X_train, ohe_y_train, validation_data=(X_valid,ohe_y_valid),\n",
    "                    epochs=5, batch_size=50,\n",
    "                    callbacks=callbacks, verbose=2)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df_history = pd.DataFrame(history.history)\n",
    "    df_history.to_csv(f\"{output_dir}/training_history.csv\", index=False)\n",
    "\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    # Salvataggio dei risultati\n",
    "    soft_values = [\",\".join(map(str, probs)) for probs in y_pred_probs]\n",
    "    df_soft = pd.DataFrame({\n",
    "        \"Actual\": y_test,\n",
    "        \"soft_values\": soft_values\n",
    "    })\n",
    "    df_pred = pd.DataFrame({\n",
    "        \"Actual\": y_test,\n",
    "        \"Predicted\": y_pred\n",
    "    })\n",
    "\n",
    "    df_soft.to_csv(f\"{output_dir}/soft_values.dat\", sep=\"\\t\", index=False)\n",
    "    df_pred.to_csv(f\"{output_dir}/predictions.dat\", sep=\"\\t\", index=False)\n",
    "    labels_map = {}\n",
    "    for c, enc_c in zip(le.classes_, le.transform(le.classes_)):\n",
    "        labels_map[str(enc_c)] = c\n",
    "    with open(f\"{output_dir}/labels_map.json\", 'w') as f:\n",
    "        json.dump(labels_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "for i in range(0,2):\n",
    "    seed = 2025 + i\n",
    "    def ingest_dataset(path, n_pkts=10, n_features=4):\n",
    "        \"\"\"Carica dataset da pickle e seleziona i primi n_pkts e n_features.\"\"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            biflows = pickle.load(f)\n",
    "            labels = pickle.load(f)\n",
    "        biflows = np.array(biflows)[:, :n_pkts, :n_features]\n",
    "        return biflows, labels\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "\n",
    "        dataset_path = \"mirage2019_LOPEZ_lopez_lopez_36P_4F_APP_xST_PAD_metadata.pickle\"\n",
    "        output_dir = \"output5/\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        n_pkts = 10\n",
    "        n_features = 4\n",
    "        sample_train_size = 50000\n",
    "        epoche = epoche\n",
    "        seed = seed\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "        X, y = ingest_dataset(dataset_path, n_pkts=n_pkts, n_features=n_features)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        num_classes = len(np.unique(y))\n",
    "\n",
    "        X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=seed\n",
    "        )\n",
    "\n",
    "        if sample_train_size < len(X_train_full):\n",
    "            X_train_full, _, y_train_full, _ = train_test_split(\n",
    "                X_train_full, y_train_full,\n",
    "                train_size=sample_train_size,\n",
    "                stratify=y_train_full,\n",
    "                random_state=seed\n",
    "            )\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_train_full_scaled = scaler.fit_transform(X_train_full.reshape(-1, n_features)).reshape(-1, n_pkts, n_features)\n",
    "        X_test_scaled = scaler.transform(X_test.reshape(-1, n_features)).reshape(-1, n_pkts, n_features)\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_train_full_scaled, y_train_full,\n",
    "            test_size=0.2, stratify=y_train_full, random_state=seed\n",
    "        )\n",
    "\n",
    "        ohe_y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "        ohe_y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=num_classes)\n",
    "\n",
    "        model = Sequential([\n",
    "            Conv2D(32, (4, 2), padding='same', activation='relu', input_shape=(n_pkts, n_features, 1)),\n",
    "            MaxPooling2D((3, 2), padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(64, (4, 2), padding='same', activation='relu'),\n",
    "            MaxPooling2D((3, 1), padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        earlystop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "        with open(f\"{output_dir}/model_summary.txt\", \"w\") as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "        X_train_cnn = X_train.reshape(-1, n_pkts, n_features, 1)\n",
    "        X_valid_cnn = X_valid.reshape(-1, n_pkts, n_features, 1)\n",
    "        X_test_cnn  = X_test_scaled.reshape(-1, n_pkts, n_features, 1)\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train_cnn, ohe_y_train,\n",
    "            validation_data=(X_valid_cnn, ohe_y_valid),\n",
    "            epochs=epoche,\n",
    "            batch_size=50,\n",
    "            callbacks=[earlystop],\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        pd.DataFrame(history.history).to_csv(f\"{output_dir}/training_history{i}.csv\", index=False)\n",
    "\n",
    "        y_pred_probs = model.predict(X_test_cnn)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "        df_soft = pd.DataFrame({\n",
    "            \"Actual\": y_test,\n",
    "            \"soft_values\": [\",\".join(map(str, p)) for p in y_pred_probs]\n",
    "        })\n",
    "        df_soft.to_csv(f\"{output_dir}/soft_values{i}.dat\", sep=\"\\t\", index=False)\n",
    "\n",
    "        df_pred = pd.DataFrame({\n",
    "            \"Actual\": y_test,\n",
    "            \"Predicted\": y_pred\n",
    "        })\n",
    "        df_pred.to_csv(f\"{output_dir}/predictions{i}.dat\", sep=\"\\t\", index=False)\n",
    "\n",
    "        labels_map = {str(enc): cls for cls, enc in zip(le.classes_, le.transform(le.classes_))}\n",
    "        with open(f\"{output_dir}/labels_map{i}.json\", \"w\") as f:\n",
    "            json.dump(labels_map, f)\n",
    "\n",
    "        print(\"✓ Training completato e file salvati.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d377f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "160/160 - 31s - loss: 3.6249 - accuracy: 0.0535 - val_loss: 3.5591 - val_accuracy: 0.0585 - 31s/epoch - 196ms/step\n",
      "Epoch 2/5\n",
      "160/160 - 31s - loss: 3.5100 - accuracy: 0.0945 - val_loss: 3.4671 - val_accuracy: 0.0995 - 31s/epoch - 194ms/step\n",
      "Epoch 3/5\n",
      "160/160 - 31s - loss: 3.4425 - accuracy: 0.0981 - val_loss: 3.4203 - val_accuracy: 0.0980 - 31s/epoch - 194ms/step\n",
      "Epoch 4/5\n",
      "160/160 - 31s - loss: 3.4068 - accuracy: 0.0972 - val_loss: 3.3912 - val_accuracy: 0.0945 - 31s/epoch - 196ms/step\n",
      "Epoch 5/5\n",
      "160/160 - 32s - loss: 3.3756 - accuracy: 0.1059 - val_loss: 3.3574 - val_accuracy: 0.1170 - 32s/epoch - 200ms/step\n",
      "607/607 [==============================] - 41s 67ms/step\n",
      "✓ QML training completato (struttura path CNN).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pennylane import numpy as np\n",
    "import pennylane as qml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# ===============================\n",
    "# Funzione caricamento dataset\n",
    "# ===============================\n",
    "def ingest_dataset(path, n_pkts=10, n_features=4):\n",
    "    with open(path, \"rb\") as f:\n",
    "        biflows = pickle.load(f)\n",
    "        labels = pickle.load(f)\n",
    "    biflows = np.array(biflows)[:, :n_pkts, :n_features]\n",
    "    return biflows, labels\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ===============================\n",
    "    # PATH COME NEL CNN MODEL\n",
    "    # ===============================\n",
    "    dataset_path = \"../mirage2019_LOPEZ_lopez_lopez_36P_4F_APP_xST_PAD_metadata.pickle\"\n",
    "    output_dir = \"quantum/output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Parametri dataset\n",
    "    n_pkts = 10\n",
    "    n_features = 4\n",
    "    sample_train_size = 10000\n",
    "    epoche = 5\n",
    "\n",
    "    # Quantum params\n",
    "    n_qubits = 5\n",
    "    n_layers = 3\n",
    "\n",
    "    # Seed globale (non modificato)\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    # ===============================\n",
    "    # CARICAMENTO\n",
    "    # ===============================\n",
    "    X, y = ingest_dataset(dataset_path, n_pkts=n_pkts, n_features=n_features)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    num_classes = len(np.unique(y))\n",
    "\n",
    "    # ===============================\n",
    "    # TRAIN / TEST\n",
    "    # ===============================\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=seed\n",
    "    )\n",
    "\n",
    "    # Sampling\n",
    "    if sample_train_size < len(X_train_full):\n",
    "        X_train_full, _, y_train_full, _ = train_test_split(\n",
    "            X_train_full, y_train_full,\n",
    "            train_size=sample_train_size,\n",
    "            stratify=y_train_full,\n",
    "            random_state=seed\n",
    "        )\n",
    "\n",
    "    # ===============================\n",
    "    # NORMALIZZAZIONE\n",
    "    # ===============================\n",
    "    scaler = MinMaxScaler((0,1))\n",
    "    X_train_full_scaled = scaler.fit_transform(\n",
    "        X_train_full.reshape(-1, n_features)\n",
    "    ).reshape(-1, n_pkts, n_features)\n",
    "\n",
    "    X_test_scaled = scaler.transform(\n",
    "        X_test.reshape(-1, n_features)\n",
    "    ).reshape(-1, n_pkts, n_features)\n",
    "\n",
    "    # ===============================\n",
    "    # TRAIN / VALID\n",
    "    # ===============================\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_train_full_scaled, y_train_full,\n",
    "        test_size=0.2, stratify=y_train_full, random_state=seed\n",
    "    )\n",
    "\n",
    "    ohe_y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    ohe_y_valid = tf.keras.utils.to_categorical(y_valid, num_classes)\n",
    "\n",
    "    # ===============================\n",
    "    # QUANTUM DEVICE\n",
    "    # ===============================\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits, seed=seed)\n",
    "\n",
    "    @qml.qnode(dev, interface=\"tf\")\n",
    "    def qnode(inputs, weights):\n",
    "        qml.AmplitudeEmbedding(inputs, wires=range(n_qubits),\n",
    "                               normalize=True, pad_with=0.0)\n",
    "        qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
    "\n",
    "    # ===============================\n",
    "    # MODELLO QUANTISTICO\n",
    "    # ===============================\n",
    "    model = Sequential(name=\"qml_stratified\")\n",
    "    model.add(Flatten(input_shape=(n_pkts, n_features)))\n",
    "    model.add(Dense(2**n_qubits, activation='sigmoid'))\n",
    "    model.add(qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=2**n_qubits))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(0.001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    earlystop = EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # ===============================\n",
    "    # TRAINING\n",
    "    # ===============================\n",
    "    history = model.fit(\n",
    "        X_train, ohe_y_train,\n",
    "        validation_data=(X_valid, ohe_y_valid),\n",
    "        epochs=epoche,\n",
    "        batch_size=50,\n",
    "        callbacks=[earlystop],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # ===============================\n",
    "    # SALVATAGGIO RISULTATI\n",
    "    # ===============================\n",
    "    pd.DataFrame(history.history).to_csv(f\"{output_dir}/training_history.csv\", index=False)\n",
    "\n",
    "    y_pred_probs = model.predict(X_test_scaled)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    df_soft = pd.DataFrame({\n",
    "        \"Actual\": y_test,\n",
    "        \"soft_values\": [\",\".join(map(str, p)) for p in y_pred_probs]\n",
    "    })\n",
    "    df_soft.to_csv(f\"{output_dir}/soft_values.dat\", sep=\"\\t\", index=False)\n",
    "\n",
    "    df_pred = pd.DataFrame({\n",
    "        \"Actual\": y_test,\n",
    "        \"Predicted\": y_pred\n",
    "    })\n",
    "    df_pred.to_csv(f\"{output_dir}/predictions.dat\", sep=\"\\t\", index=False)\n",
    "\n",
    "    labels_map = {str(enc): cls for cls, enc in zip(le.classes_, le.transform(le.classes_))}\n",
    "    with open(f\"{output_dir}/labels_map.json\", \"w\") as f:\n",
    "        json.dump(labels_map, f)\n",
    "\n",
    "    print(\"✓ QML training completato (struttura path CNN).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
