{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ebac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoche = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13b28f",
   "metadata": {},
   "source": [
    "# Addestramento di benchmarking su dataset originale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89ec663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1242/1242 - 15s - loss: 2.0537 - accuracy: 0.4548 - val_loss: 1.7157 - val_accuracy: 0.5336 - 15s/epoch - 12ms/step\n",
      "Epoch 2/30\n",
      "1242/1242 - 14s - loss: 1.4876 - accuracy: 0.5857 - val_loss: 1.5141 - val_accuracy: 0.5824 - 14s/epoch - 12ms/step\n",
      "Epoch 3/30\n",
      "1242/1242 - 13s - loss: 1.2844 - accuracy: 0.6350 - val_loss: 1.4380 - val_accuracy: 0.5997 - 13s/epoch - 11ms/step\n",
      "Epoch 4/30\n",
      "1242/1242 - 13s - loss: 1.1629 - accuracy: 0.6635 - val_loss: 1.3231 - val_accuracy: 0.6347 - 13s/epoch - 11ms/step\n",
      "Epoch 5/30\n",
      "1242/1242 - 13s - loss: 1.0846 - accuracy: 0.6824 - val_loss: 1.3121 - val_accuracy: 0.6333 - 13s/epoch - 11ms/step\n",
      "Epoch 6/30\n",
      "1242/1242 - 13s - loss: 1.0155 - accuracy: 0.6997 - val_loss: 1.2529 - val_accuracy: 0.6566 - 13s/epoch - 11ms/step\n",
      "Epoch 7/30\n",
      "1242/1242 - 13s - loss: 0.9592 - accuracy: 0.7132 - val_loss: 1.2127 - val_accuracy: 0.6672 - 13s/epoch - 11ms/step\n",
      "Epoch 8/30\n",
      "1242/1242 - 13s - loss: 0.9134 - accuracy: 0.7254 - val_loss: 1.1398 - val_accuracy: 0.6864 - 13s/epoch - 11ms/step\n",
      "Epoch 9/30\n",
      "1242/1242 - 13s - loss: 0.8758 - accuracy: 0.7357 - val_loss: 1.1437 - val_accuracy: 0.6912 - 13s/epoch - 10ms/step\n",
      "Epoch 10/30\n",
      "1242/1242 - 13s - loss: 0.8447 - accuracy: 0.7441 - val_loss: 1.1510 - val_accuracy: 0.6907 - 13s/epoch - 10ms/step\n",
      "Epoch 11/30\n",
      "1242/1242 - 14s - loss: 0.8158 - accuracy: 0.7499 - val_loss: 1.1475 - val_accuracy: 0.6894 - 14s/epoch - 11ms/step\n",
      "Epoch 12/30\n",
      "1242/1242 - 13s - loss: 0.7838 - accuracy: 0.7596 - val_loss: 1.1623 - val_accuracy: 0.6917 - 13s/epoch - 11ms/step\n",
      "Epoch 13/30\n",
      "1242/1242 - 14s - loss: 0.7612 - accuracy: 0.7661 - val_loss: 1.1162 - val_accuracy: 0.7012 - 14s/epoch - 12ms/step\n",
      "Epoch 14/30\n",
      "1242/1242 - 14s - loss: 0.7419 - accuracy: 0.7725 - val_loss: 1.1035 - val_accuracy: 0.7141 - 14s/epoch - 12ms/step\n",
      "Epoch 15/30\n",
      "1242/1242 - 15s - loss: 0.7203 - accuracy: 0.7774 - val_loss: 1.2586 - val_accuracy: 0.6749 - 15s/epoch - 12ms/step\n",
      "Epoch 16/30\n",
      "1242/1242 - 15s - loss: 0.7013 - accuracy: 0.7840 - val_loss: 1.2420 - val_accuracy: 0.6868 - 15s/epoch - 12ms/step\n",
      "Epoch 17/30\n",
      "1242/1242 - 14s - loss: 0.6864 - accuracy: 0.7864 - val_loss: 1.1584 - val_accuracy: 0.7075 - 14s/epoch - 12ms/step\n",
      "Epoch 18/30\n",
      "1242/1242 - 15s - loss: 0.6673 - accuracy: 0.7910 - val_loss: 1.1499 - val_accuracy: 0.7200 - 15s/epoch - 12ms/step\n",
      "Epoch 19/30\n",
      "1242/1242 - 14s - loss: 0.6543 - accuracy: 0.7956 - val_loss: 1.0513 - val_accuracy: 0.7398 - 14s/epoch - 12ms/step\n",
      "Epoch 20/30\n",
      "1242/1242 - 14s - loss: 0.6429 - accuracy: 0.7996 - val_loss: 1.0587 - val_accuracy: 0.7437 - 14s/epoch - 11ms/step\n",
      "Epoch 21/30\n",
      "1242/1242 - 14s - loss: 0.6244 - accuracy: 0.8037 - val_loss: 1.1281 - val_accuracy: 0.7338 - 14s/epoch - 12ms/step\n",
      "Epoch 22/30\n",
      "1242/1242 - 14s - loss: 0.6193 - accuracy: 0.8052 - val_loss: 1.2238 - val_accuracy: 0.6880 - 14s/epoch - 12ms/step\n",
      "Epoch 23/30\n",
      "1242/1242 - 14s - loss: 0.6042 - accuracy: 0.8089 - val_loss: 1.1134 - val_accuracy: 0.7334 - 14s/epoch - 12ms/step\n",
      "Epoch 24/30\n",
      "1242/1242 - 13s - loss: 0.5972 - accuracy: 0.8112 - val_loss: 1.1713 - val_accuracy: 0.7014 - 13s/epoch - 11ms/step\n",
      "Epoch 25/30\n",
      "1242/1242 - 13s - loss: 0.5879 - accuracy: 0.8148 - val_loss: 1.0545 - val_accuracy: 0.7450 - 13s/epoch - 11ms/step\n",
      "Epoch 26/30\n",
      "1242/1242 - 13s - loss: 0.5779 - accuracy: 0.8161 - val_loss: 1.1414 - val_accuracy: 0.7242 - 13s/epoch - 11ms/step\n",
      "Epoch 27/30\n",
      "1242/1242 - 13s - loss: 0.5687 - accuracy: 0.8175 - val_loss: 1.1404 - val_accuracy: 0.7247 - 13s/epoch - 11ms/step\n",
      "Epoch 28/30\n",
      "1242/1242 - 13s - loss: 0.5649 - accuracy: 0.8199 - val_loss: 1.1129 - val_accuracy: 0.7428 - 13s/epoch - 11ms/step\n",
      "Epoch 29/30\n",
      "1242/1242 - 14s - loss: 0.5492 - accuracy: 0.8242 - val_loss: 1.1183 - val_accuracy: 0.7516 - 14s/epoch - 12ms/step\n",
      "Epoch 30/30\n",
      "1242/1242 - 14s - loss: 0.5485 - accuracy: 0.8248 - val_loss: 1.2081 - val_accuracy: 0.7065 - 14s/epoch - 11ms/step\n",
      "607/607 [==============================] - 2s 2ms/step\n",
      "Epoch 1/30\n",
      "1242/1242 - 14s - loss: 2.0955 - accuracy: 0.4464 - val_loss: 1.7624 - val_accuracy: 0.5124 - 14s/epoch - 11ms/step\n",
      "Epoch 2/30\n",
      "1242/1242 - 14s - loss: 1.5103 - accuracy: 0.5815 - val_loss: 1.4994 - val_accuracy: 0.5836 - 14s/epoch - 12ms/step\n",
      "Epoch 3/30\n",
      "1242/1242 - 15s - loss: 1.3074 - accuracy: 0.6304 - val_loss: 1.3480 - val_accuracy: 0.6286 - 15s/epoch - 12ms/step\n",
      "Epoch 4/30\n",
      "1242/1242 - 15s - loss: 1.1899 - accuracy: 0.6588 - val_loss: 1.2757 - val_accuracy: 0.6399 - 15s/epoch - 12ms/step\n",
      "Epoch 5/30\n",
      "1242/1242 - 15s - loss: 1.0989 - accuracy: 0.6794 - val_loss: 1.2148 - val_accuracy: 0.6651 - 15s/epoch - 12ms/step\n",
      "Epoch 6/30\n",
      "1242/1242 - 15s - loss: 1.0330 - accuracy: 0.6959 - val_loss: 1.2549 - val_accuracy: 0.6568 - 15s/epoch - 12ms/step\n",
      "Epoch 7/30\n",
      "1242/1242 - 15s - loss: 0.9753 - accuracy: 0.7086 - val_loss: 1.2899 - val_accuracy: 0.6421 - 15s/epoch - 12ms/step\n",
      "Epoch 8/30\n",
      "1242/1242 - 15s - loss: 0.9277 - accuracy: 0.7215 - val_loss: 1.1954 - val_accuracy: 0.6673 - 15s/epoch - 12ms/step\n",
      "Epoch 9/30\n",
      "1242/1242 - 15s - loss: 0.8877 - accuracy: 0.7327 - val_loss: 1.1655 - val_accuracy: 0.6772 - 15s/epoch - 12ms/step\n",
      "Epoch 10/30\n",
      "1242/1242 - 13s - loss: 0.8522 - accuracy: 0.7403 - val_loss: 1.1598 - val_accuracy: 0.6826 - 13s/epoch - 11ms/step\n",
      "Epoch 11/30\n",
      "1242/1242 - 13s - loss: 0.8186 - accuracy: 0.7512 - val_loss: 1.1510 - val_accuracy: 0.6895 - 13s/epoch - 11ms/step\n",
      "Epoch 12/30\n",
      "1242/1242 - 13s - loss: 0.7914 - accuracy: 0.7566 - val_loss: 1.0410 - val_accuracy: 0.7198 - 13s/epoch - 11ms/step\n",
      "Epoch 13/30\n",
      "1242/1242 - 13s - loss: 0.7679 - accuracy: 0.7639 - val_loss: 1.1135 - val_accuracy: 0.7046 - 13s/epoch - 11ms/step\n",
      "Epoch 14/30\n",
      "1242/1242 - 14s - loss: 0.7470 - accuracy: 0.7695 - val_loss: 1.1330 - val_accuracy: 0.7057 - 14s/epoch - 11ms/step\n",
      "Epoch 15/30\n",
      "1242/1242 - 14s - loss: 0.7276 - accuracy: 0.7758 - val_loss: 1.1312 - val_accuracy: 0.7053 - 14s/epoch - 12ms/step\n",
      "Epoch 16/30\n",
      "1242/1242 - 13s - loss: 0.7109 - accuracy: 0.7799 - val_loss: 1.0707 - val_accuracy: 0.7236 - 13s/epoch - 11ms/step\n",
      "Epoch 17/30\n",
      "1242/1242 - 14s - loss: 0.6902 - accuracy: 0.7850 - val_loss: 1.0908 - val_accuracy: 0.7213 - 14s/epoch - 11ms/step\n",
      "Epoch 18/30\n",
      "1242/1242 - 14s - loss: 0.6818 - accuracy: 0.7865 - val_loss: 1.1214 - val_accuracy: 0.7121 - 14s/epoch - 11ms/step\n",
      "Epoch 19/30\n",
      "1242/1242 - 14s - loss: 0.6647 - accuracy: 0.7915 - val_loss: 1.0768 - val_accuracy: 0.7286 - 14s/epoch - 11ms/step\n",
      "Epoch 20/30\n",
      "1242/1242 - 15s - loss: 0.6470 - accuracy: 0.7966 - val_loss: 1.0515 - val_accuracy: 0.7376 - 15s/epoch - 12ms/step\n",
      "Epoch 21/30\n",
      "1242/1242 - 14s - loss: 0.6382 - accuracy: 0.7990 - val_loss: 1.1571 - val_accuracy: 0.7068 - 14s/epoch - 11ms/step\n",
      "Epoch 22/30\n",
      "1242/1242 - 13s - loss: 0.6270 - accuracy: 0.8021 - val_loss: 1.0291 - val_accuracy: 0.7474 - 13s/epoch - 11ms/step\n",
      "Epoch 23/30\n",
      "1242/1242 - 14s - loss: 0.6143 - accuracy: 0.8069 - val_loss: 1.0871 - val_accuracy: 0.7380 - 14s/epoch - 11ms/step\n",
      "Epoch 24/30\n",
      "1242/1242 - 14s - loss: 0.6072 - accuracy: 0.8069 - val_loss: 1.0763 - val_accuracy: 0.7321 - 14s/epoch - 12ms/step\n",
      "Epoch 25/30\n",
      "1242/1242 - 15s - loss: 0.5988 - accuracy: 0.8110 - val_loss: 1.1204 - val_accuracy: 0.7264 - 15s/epoch - 12ms/step\n",
      "Epoch 26/30\n",
      "1242/1242 - 14s - loss: 0.5899 - accuracy: 0.8129 - val_loss: 1.0272 - val_accuracy: 0.7468 - 14s/epoch - 11ms/step\n",
      "Epoch 27/30\n",
      "1242/1242 - 14s - loss: 0.5782 - accuracy: 0.8150 - val_loss: 1.0290 - val_accuracy: 0.7512 - 14s/epoch - 11ms/step\n",
      "Epoch 28/30\n",
      "1242/1242 - 14s - loss: 0.5699 - accuracy: 0.8177 - val_loss: 1.1297 - val_accuracy: 0.7278 - 14s/epoch - 11ms/step\n",
      "Epoch 29/30\n",
      "1242/1242 - 14s - loss: 0.5637 - accuracy: 0.8197 - val_loss: 1.2232 - val_accuracy: 0.6975 - 14s/epoch - 11ms/step\n",
      "Epoch 30/30\n",
      "1242/1242 - 14s - loss: 0.5543 - accuracy: 0.8224 - val_loss: 1.1501 - val_accuracy: 0.7330 - 14s/epoch - 11ms/step\n",
      "607/607 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# def get_args():\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         description=\"Parse the script arguments.\"\n",
    "#     )\n",
    "\n",
    "#     parser.add_argument(\n",
    "#         \"mirage2019_LOPEZ_lopez_lopez_36P_4F_APP_xST_PAD_metadata.pickle\",\n",
    "#         type=str,\n",
    "#         required=True,\n",
    "#         help=\"--dataset-path\"\n",
    "#     )\n",
    "\n",
    "#     parser.add_argument(\n",
    "#         \"output\",\n",
    "#         type=str,\n",
    "#         required=True,\n",
    "#         help=\"--output-dir\"\n",
    "#     )\n",
    "\n",
    "#     return parser.parse_args()\n",
    "\n",
    "for i in range(0,2):\n",
    "    seed = 2025 + i\n",
    "    def ingest_dataset(path, n_pkts=10, n_features=4):\n",
    "        with open(path, \"rb\") as f:\n",
    "            biflows = pickle.load(f)\n",
    "            labels = pickle.load(f)\n",
    "        biflows = np.array(biflows)[:,:n_pkts,:n_features]\n",
    "        return biflows, labels\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # Parsing degli argomenti\n",
    "        # args = get_args()\n",
    "        dataset_path = \"mirage2019_LOPEZ_lopez_lopez_36P_4F_APP_xST_PAD_metadata.pickle\"\n",
    "        output_dir = \"output/\"\n",
    "\n",
    "        n_pkts = 10\n",
    "        \n",
    "        n_features = 4\n",
    "        seed = seed\n",
    "\n",
    "        # Riproducibilità\n",
    "        np.random.seed(seed)                    # NumPy\n",
    "        tf.random.set_seed(seed)                # TensorFlow\n",
    "        tf.keras.utils.set_random_seed(seed)    # Keras\n",
    "\n",
    "        # Caricamento del dataset\n",
    "        X, y = ingest_dataset(dataset_path, n_pkts=n_pkts, n_features=n_features)\n",
    "        num_classes = len(np.unique(y))\n",
    "\n",
    "        # Codifica delle label\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "        # Partizionamento in train, validation e test set (proporzioni 80/20, 80/20)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=seed, stratify=y,\n",
    "        )\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler.fit(np.reshape(X_train, [-1, n_features]))\n",
    "        res_samples_train = scaler.transform(np.reshape(X_train, [-1, n_features]))\n",
    "        res_samples_test = scaler.transform(np.reshape(X_test, [-1, n_features]))\n",
    "        X_train = np.reshape(res_samples_train, [-1, n_pkts, n_features])\n",
    "        X_test = np.reshape(res_samples_test, [-1, n_pkts, n_features])\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=seed, stratify=y_train,\n",
    "        )\n",
    "\n",
    "        ohe_y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "        ohe_y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=num_classes)\n",
    "\n",
    "        # Definizione del modello (questa parte si può ri-modulare)\n",
    "        model = Sequential(name='lopez2017network_CNN_1')\n",
    "        model.add(Conv2D(filters=32, kernel_size=(4, 2), strides=1, padding='same', activation='relu',\n",
    "                            input_shape=(n_pkts, n_features, 1)))\n",
    "        model.add(MaxPooling2D(pool_size=(3, 2), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=64, kernel_size=(4, 2), strides=1, padding='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3, 1), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(200, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "        callbacks = [earlystop]\n",
    "\n",
    "        with open(f\"{output_dir}/model_summary.txt\", \"w\") as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "        history = model.fit(X_train, ohe_y_train, validation_data=(X_valid,ohe_y_valid),\n",
    "                        epochs=epoche, batch_size=50,\n",
    "                        callbacks=callbacks, verbose=2)\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df_history = pd.DataFrame(history.history)\n",
    "        df_history.to_csv(f\"{output_dir}/training_history{i}.csv\", index=False)\n",
    "\n",
    "        y_pred_probs = model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "        # Salvataggio dei risultati\n",
    "        soft_values = [\",\".join(map(str, probs)) for probs in y_pred_probs]\n",
    "        df_soft = pd.DataFrame({\n",
    "            \"Actual\": y_test,\n",
    "            \"soft_values\": soft_values\n",
    "        })\n",
    "        df_pred = pd.DataFrame({\n",
    "            \"Actual\": y_test,\n",
    "            \"Predicted\": y_pred\n",
    "        })\n",
    "    \n",
    "        df_soft.to_csv(f\"{output_dir}/soft_values{i}.dat\", sep=\"\\t\", index=False)\n",
    "        df_pred.to_csv(f\"{output_dir}/predictions{i}.dat\", sep=\"\\t\", index=False)\n",
    "        labels_map = {}\n",
    "        for c, enc_c in zip(le.classes_, le.transform(le.classes_)):\n",
    "            labels_map[str(enc_c)] = c\n",
    "        with open(f\"{output_dir}/labels_map{i}.json\", 'w') as f:\n",
    "            json.dump(labels_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6985b8",
   "metadata": {},
   "source": [
    "# Addestramento di benchmarking su dataset ridimensinato a 10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d69746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "160/160 - 2s - loss: 2.8314 - accuracy: 0.2699 - val_loss: 3.5347 - val_accuracy: 0.0835 - 2s/epoch - 9ms/step\n",
      "Epoch 2/30\n",
      "160/160 - 1s - loss: 2.2595 - accuracy: 0.4049 - val_loss: 3.6440 - val_accuracy: 0.0840 - 694ms/epoch - 4ms/step\n",
      "Epoch 3/30\n",
      "160/160 - 1s - loss: 2.0035 - accuracy: 0.4618 - val_loss: 3.1257 - val_accuracy: 0.1975 - 619ms/epoch - 4ms/step\n",
      "Epoch 4/30\n",
      "160/160 - 1s - loss: 1.8078 - accuracy: 0.5082 - val_loss: 2.4069 - val_accuracy: 0.3690 - 652ms/epoch - 4ms/step\n",
      "Epoch 5/30\n",
      "160/160 - 1s - loss: 1.6905 - accuracy: 0.5305 - val_loss: 2.0743 - val_accuracy: 0.4650 - 621ms/epoch - 4ms/step\n",
      "Epoch 6/30\n",
      "160/160 - 1s - loss: 1.5606 - accuracy: 0.5669 - val_loss: 1.9800 - val_accuracy: 0.4980 - 654ms/epoch - 4ms/step\n",
      "Epoch 7/30\n",
      "160/160 - 1s - loss: 1.4676 - accuracy: 0.5811 - val_loss: 1.9686 - val_accuracy: 0.5035 - 644ms/epoch - 4ms/step\n",
      "Epoch 8/30\n",
      "160/160 - 1s - loss: 1.3770 - accuracy: 0.6085 - val_loss: 2.0615 - val_accuracy: 0.4885 - 654ms/epoch - 4ms/step\n",
      "Epoch 9/30\n",
      "160/160 - 1s - loss: 1.3025 - accuracy: 0.6224 - val_loss: 1.9564 - val_accuracy: 0.5180 - 640ms/epoch - 4ms/step\n",
      "Epoch 10/30\n",
      "160/160 - 1s - loss: 1.2372 - accuracy: 0.6406 - val_loss: 1.9080 - val_accuracy: 0.5345 - 646ms/epoch - 4ms/step\n",
      "Epoch 11/30\n",
      "160/160 - 1s - loss: 1.1793 - accuracy: 0.6543 - val_loss: 1.9452 - val_accuracy: 0.5290 - 649ms/epoch - 4ms/step\n",
      "Epoch 12/30\n",
      "160/160 - 1s - loss: 1.1253 - accuracy: 0.6654 - val_loss: 1.9243 - val_accuracy: 0.5390 - 655ms/epoch - 4ms/step\n",
      "Epoch 13/30\n",
      "160/160 - 1s - loss: 1.0776 - accuracy: 0.6762 - val_loss: 1.9479 - val_accuracy: 0.5340 - 645ms/epoch - 4ms/step\n",
      "Epoch 14/30\n",
      "160/160 - 1s - loss: 1.0356 - accuracy: 0.6925 - val_loss: 1.9881 - val_accuracy: 0.5460 - 669ms/epoch - 4ms/step\n",
      "Epoch 15/30\n",
      "160/160 - 1s - loss: 0.9936 - accuracy: 0.7026 - val_loss: 1.9802 - val_accuracy: 0.5605 - 678ms/epoch - 4ms/step\n",
      "Epoch 16/30\n",
      "160/160 - 1s - loss: 0.9399 - accuracy: 0.7179 - val_loss: 1.9166 - val_accuracy: 0.5660 - 669ms/epoch - 4ms/step\n",
      "Epoch 17/30\n",
      "160/160 - 1s - loss: 0.9241 - accuracy: 0.7191 - val_loss: 2.0121 - val_accuracy: 0.5445 - 663ms/epoch - 4ms/step\n",
      "Epoch 18/30\n",
      "160/160 - 1s - loss: 0.8842 - accuracy: 0.7297 - val_loss: 2.0229 - val_accuracy: 0.5640 - 659ms/epoch - 4ms/step\n",
      "Epoch 19/30\n",
      "160/160 - 1s - loss: 0.8627 - accuracy: 0.7366 - val_loss: 2.1561 - val_accuracy: 0.5315 - 634ms/epoch - 4ms/step\n",
      "Epoch 20/30\n",
      "160/160 - 1s - loss: 0.8266 - accuracy: 0.7464 - val_loss: 2.0871 - val_accuracy: 0.5615 - 611ms/epoch - 4ms/step\n",
      "Epoch 21/30\n",
      "160/160 - 1s - loss: 0.8066 - accuracy: 0.7480 - val_loss: 2.0841 - val_accuracy: 0.5655 - 646ms/epoch - 4ms/step\n",
      "Epoch 22/30\n",
      "160/160 - 1s - loss: 0.7770 - accuracy: 0.7573 - val_loss: 2.1585 - val_accuracy: 0.5475 - 645ms/epoch - 4ms/step\n",
      "Epoch 23/30\n",
      "160/160 - 1s - loss: 0.7504 - accuracy: 0.7655 - val_loss: 2.1321 - val_accuracy: 0.5585 - 647ms/epoch - 4ms/step\n",
      "Epoch 24/30\n",
      "160/160 - 1s - loss: 0.7510 - accuracy: 0.7632 - val_loss: 2.1319 - val_accuracy: 0.5575 - 657ms/epoch - 4ms/step\n",
      "Epoch 25/30\n",
      "160/160 - 1s - loss: 0.7288 - accuracy: 0.7695 - val_loss: 2.1578 - val_accuracy: 0.5710 - 634ms/epoch - 4ms/step\n",
      "Epoch 26/30\n",
      "160/160 - 1s - loss: 0.7088 - accuracy: 0.7784 - val_loss: 2.0710 - val_accuracy: 0.5895 - 648ms/epoch - 4ms/step\n",
      "Epoch 27/30\n",
      "160/160 - 1s - loss: 0.6837 - accuracy: 0.7859 - val_loss: 2.1930 - val_accuracy: 0.5650 - 609ms/epoch - 4ms/step\n",
      "Epoch 28/30\n",
      "160/160 - 1s - loss: 0.6719 - accuracy: 0.7857 - val_loss: 2.2556 - val_accuracy: 0.5555 - 607ms/epoch - 4ms/step\n",
      "Epoch 29/30\n",
      "160/160 - 1s - loss: 0.6617 - accuracy: 0.7943 - val_loss: 2.1770 - val_accuracy: 0.5865 - 599ms/epoch - 4ms/step\n",
      "Epoch 30/30\n",
      "160/160 - 1s - loss: 0.6426 - accuracy: 0.7921 - val_loss: 2.3815 - val_accuracy: 0.5485 - 611ms/epoch - 4ms/step\n",
      "607/607 [==============================] - 1s 2ms/step\n",
      "✓ Training completato e file salvati.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def ingest_dataset(path, n_pkts=10, n_features=4):\n",
    "    \"\"\"Carica dataset da pickle e seleziona i primi n_pkts e n_features.\"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        biflows = pickle.load(f)\n",
    "        labels = pickle.load(f)\n",
    "    biflows = np.array(biflows)[:, :n_pkts, :n_features]\n",
    "    return biflows, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataset_path = \"mirage2019_LOPEZ_lopez_lopez_36P_4F_APP_xST_PAD_metadata.pickle\"\n",
    "    output_dir = \"output2/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    n_pkts = 10\n",
    "    n_features = 4\n",
    "    sample_train_size = 10000    \n",
    "    epoche = epoche\n",
    "    seed = 2025\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    X, y = ingest_dataset(dataset_path, n_pkts=n_pkts, n_features=n_features)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    num_classes = len(np.unique(y))\n",
    "\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=seed\n",
    "    )\n",
    "\n",
    "    if sample_train_size < len(X_train_full):\n",
    "        X_train_full, _, y_train_full, _ = train_test_split(\n",
    "            X_train_full, y_train_full,\n",
    "            train_size=sample_train_size,\n",
    "            stratify=y_train_full,\n",
    "            random_state=seed\n",
    "        )\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train_full_scaled = scaler.fit_transform(X_train_full.reshape(-1, n_features)).reshape(-1, n_pkts, n_features)\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, n_features)).reshape(-1, n_pkts, n_features)\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_train_full_scaled, y_train_full,\n",
    "        test_size=0.2, stratify=y_train_full, random_state=seed\n",
    "    )\n",
    "\n",
    "    ohe_y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "    ohe_y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=num_classes)\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (4, 2), padding='same', activation='relu', input_shape=(n_pkts, n_features, 1)),\n",
    "        MaxPooling2D((3, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (4, 2), padding='same', activation='relu'),\n",
    "        MaxPooling2D((3, 1), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    earlystop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "    with open(f\"{output_dir}/model_summary.txt\", \"w\") as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "    X_train_cnn = X_train.reshape(-1, n_pkts, n_features, 1)\n",
    "    X_valid_cnn = X_valid.reshape(-1, n_pkts, n_features, 1)\n",
    "    X_test_cnn  = X_test_scaled.reshape(-1, n_pkts, n_features, 1)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_cnn, ohe_y_train,\n",
    "        validation_data=(X_valid_cnn, ohe_y_valid),\n",
    "        epochs=epoche,\n",
    "        batch_size=50,\n",
    "        callbacks=[earlystop],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    pd.DataFrame(history.history).to_csv(f\"{output_dir}/training_history.csv\", index=False)\n",
    "\n",
    "    y_pred_probs = model.predict(X_test_cnn)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    df_soft = pd.DataFrame({\n",
    "        \"Actual\": y_test,\n",
    "        \"soft_values\": [\",\".join(map(str, p)) for p in y_pred_probs]\n",
    "    })\n",
    "    df_soft.to_csv(f\"{output_dir}/soft_values.dat\", sep=\"\\t\", index=False)\n",
    "\n",
    "    df_pred = pd.DataFrame({\n",
    "        \"Actual\": y_test,\n",
    "        \"Predicted\": y_pred\n",
    "    })\n",
    "    df_pred.to_csv(f\"{output_dir}/predictions.dat\", sep=\"\\t\", index=False)\n",
    "\n",
    "    labels_map = {str(enc): cls for cls, enc in zip(le.classes_, le.transform(le.classes_))}\n",
    "    with open(f\"{output_dir}/labels_map.json\", \"w\") as f:\n",
    "        json.dump(labels_map, f)\n",
    "\n",
    "    print(\"✓ Training completato e file salvati.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe5806",
   "metadata": {},
   "source": [
    "# Addestramento di benchmarking su dataset ridimensinato a 30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3e02a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "480/480 - 3s - loss: 2.4303 - accuracy: 0.3709 - val_loss: 2.8603 - val_accuracy: 0.2582 - 3s/epoch - 6ms/step\n",
      "Epoch 2/30\n",
      "480/480 - 2s - loss: 1.8583 - accuracy: 0.5011 - val_loss: 1.8224 - val_accuracy: 0.5057 - 2s/epoch - 4ms/step\n",
      "Epoch 3/30\n",
      "480/480 - 2s - loss: 1.6418 - accuracy: 0.5480 - val_loss: 1.6411 - val_accuracy: 0.5458 - 2s/epoch - 4ms/step\n",
      "Epoch 4/30\n",
      "480/480 - 2s - loss: 1.4936 - accuracy: 0.5839 - val_loss: 1.5951 - val_accuracy: 0.5642 - 2s/epoch - 4ms/step\n",
      "Epoch 5/30\n",
      "480/480 - 2s - loss: 1.3892 - accuracy: 0.6074 - val_loss: 1.6822 - val_accuracy: 0.5418 - 2s/epoch - 4ms/step\n",
      "Epoch 6/30\n",
      "480/480 - 2s - loss: 1.3069 - accuracy: 0.6306 - val_loss: 1.5215 - val_accuracy: 0.5867 - 2s/epoch - 4ms/step\n",
      "Epoch 7/30\n",
      "480/480 - 2s - loss: 1.2326 - accuracy: 0.6438 - val_loss: 1.4647 - val_accuracy: 0.5975 - 2s/epoch - 4ms/step\n",
      "Epoch 8/30\n",
      "480/480 - 2s - loss: 1.1784 - accuracy: 0.6562 - val_loss: 1.4188 - val_accuracy: 0.6013 - 2s/epoch - 4ms/step\n",
      "Epoch 9/30\n",
      "480/480 - 2s - loss: 1.1269 - accuracy: 0.6668 - val_loss: 1.3577 - val_accuracy: 0.6368 - 2s/epoch - 4ms/step\n",
      "Epoch 10/30\n",
      "480/480 - 2s - loss: 1.0818 - accuracy: 0.6790 - val_loss: 1.3807 - val_accuracy: 0.6313 - 2s/epoch - 4ms/step\n",
      "Epoch 11/30\n",
      "480/480 - 2s - loss: 1.0462 - accuracy: 0.6900 - val_loss: 1.5452 - val_accuracy: 0.5967 - 2s/epoch - 4ms/step\n",
      "Epoch 12/30\n",
      "480/480 - 2s - loss: 1.0107 - accuracy: 0.6941 - val_loss: 1.4605 - val_accuracy: 0.6197 - 2s/epoch - 4ms/step\n",
      "Epoch 13/30\n",
      "480/480 - 2s - loss: 0.9773 - accuracy: 0.7042 - val_loss: 1.3546 - val_accuracy: 0.6382 - 2s/epoch - 4ms/step\n",
      "Epoch 14/30\n",
      "480/480 - 2s - loss: 0.9528 - accuracy: 0.7094 - val_loss: 1.3785 - val_accuracy: 0.6315 - 2s/epoch - 4ms/step\n",
      "Epoch 15/30\n",
      "480/480 - 2s - loss: 0.9122 - accuracy: 0.7197 - val_loss: 1.4389 - val_accuracy: 0.6380 - 2s/epoch - 4ms/step\n",
      "Epoch 16/30\n",
      "480/480 - 2s - loss: 0.8989 - accuracy: 0.7247 - val_loss: 1.3688 - val_accuracy: 0.6458 - 2s/epoch - 4ms/step\n",
      "Epoch 17/30\n",
      "480/480 - 2s - loss: 0.8747 - accuracy: 0.7310 - val_loss: 1.3147 - val_accuracy: 0.6643 - 2s/epoch - 4ms/step\n",
      "Epoch 18/30\n",
      "480/480 - 2s - loss: 0.8556 - accuracy: 0.7361 - val_loss: 1.3597 - val_accuracy: 0.6518 - 2s/epoch - 4ms/step\n",
      "Epoch 19/30\n",
      "480/480 - 2s - loss: 0.8299 - accuracy: 0.7460 - val_loss: 1.3583 - val_accuracy: 0.6573 - 2s/epoch - 4ms/step\n",
      "Epoch 20/30\n",
      "480/480 - 2s - loss: 0.8218 - accuracy: 0.7433 - val_loss: 1.4013 - val_accuracy: 0.6492 - 2s/epoch - 4ms/step\n",
      "Epoch 21/30\n",
      "480/480 - 2s - loss: 0.8010 - accuracy: 0.7494 - val_loss: 1.5000 - val_accuracy: 0.6253 - 2s/epoch - 4ms/step\n",
      "Epoch 22/30\n",
      "480/480 - 2s - loss: 0.7834 - accuracy: 0.7561 - val_loss: 1.3462 - val_accuracy: 0.6625 - 2s/epoch - 4ms/step\n",
      "Epoch 23/30\n",
      "480/480 - 2s - loss: 0.7659 - accuracy: 0.7590 - val_loss: 1.3918 - val_accuracy: 0.6675 - 2s/epoch - 4ms/step\n",
      "Epoch 24/30\n",
      "480/480 - 2s - loss: 0.7513 - accuracy: 0.7626 - val_loss: 1.4189 - val_accuracy: 0.6527 - 2s/epoch - 4ms/step\n",
      "Epoch 25/30\n",
      "480/480 - 2s - loss: 0.7387 - accuracy: 0.7666 - val_loss: 1.4381 - val_accuracy: 0.6528 - 2s/epoch - 4ms/step\n",
      "Epoch 26/30\n",
      "480/480 - 2s - loss: 0.7275 - accuracy: 0.7685 - val_loss: 1.6574 - val_accuracy: 0.6053 - 2s/epoch - 4ms/step\n",
      "Epoch 27/30\n",
      "480/480 - 2s - loss: 0.7078 - accuracy: 0.7772 - val_loss: 1.3824 - val_accuracy: 0.6707 - 2s/epoch - 4ms/step\n",
      "Epoch 28/30\n",
      "480/480 - 2s - loss: 0.6999 - accuracy: 0.7762 - val_loss: 1.3676 - val_accuracy: 0.6797 - 2s/epoch - 4ms/step\n",
      "Epoch 29/30\n",
      "480/480 - 2s - loss: 0.6946 - accuracy: 0.7806 - val_loss: 1.4098 - val_accuracy: 0.6607 - 2s/epoch - 4ms/step\n",
      "Epoch 30/30\n",
      "480/480 - 2s - loss: 0.6798 - accuracy: 0.7836 - val_loss: 1.3677 - val_accuracy: 0.6720 - 2s/epoch - 4ms/step\n",
      "607/607 [==============================] - 1s 2ms/step\n",
      "✓ Training completato e file salvati.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def ingest_dataset(path, n_pkts=10, n_features=4):\n",
    "    \"\"\"Carica dataset da pickle e seleziona i primi n_pkts e n_features.\"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        biflows = pickle.load(f)\n",
    "        labels = pickle.load(f)\n",
    "    biflows = np.array(biflows)[:, :n_pkts, :n_features]\n",
    "    return biflows, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataset_path = \"mirage2019_LOPEZ_lopez_lopez_36P_4F_APP_xST_PAD_metadata.pickle\"\n",
    "    output_dir = \"output3/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    n_pkts = 10\n",
    "    n_features = 4\n",
    "    sample_train_size = 30000    \n",
    "    epoche = epoche\n",
    "    seed = 2025\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    X, y = ingest_dataset(dataset_path, n_pkts=n_pkts, n_features=n_features)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    num_classes = len(np.unique(y))\n",
    "\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=seed\n",
    "    )\n",
    "\n",
    "    if sample_train_size < len(X_train_full):\n",
    "        X_train_full, _, y_train_full, _ = train_test_split(\n",
    "            X_train_full, y_train_full,\n",
    "            train_size=sample_train_size,\n",
    "            stratify=y_train_full,\n",
    "            random_state=seed\n",
    "        )\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train_full_scaled = scaler.fit_transform(X_train_full.reshape(-1, n_features)).reshape(-1, n_pkts, n_features)\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, n_features)).reshape(-1, n_pkts, n_features)\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_train_full_scaled, y_train_full,\n",
    "        test_size=0.2, stratify=y_train_full, random_state=seed\n",
    "    )\n",
    "\n",
    "    ohe_y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "    ohe_y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=num_classes)\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (4, 2), padding='same', activation='relu', input_shape=(n_pkts, n_features, 1)),\n",
    "        MaxPooling2D((3, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (4, 2), padding='same', activation='relu'),\n",
    "        MaxPooling2D((3, 1), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    earlystop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "    with open(f\"{output_dir}/model_summary.txt\", \"w\") as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "    X_train_cnn = X_train.reshape(-1, n_pkts, n_features, 1)\n",
    "    X_valid_cnn = X_valid.reshape(-1, n_pkts, n_features, 1)\n",
    "    X_test_cnn  = X_test_scaled.reshape(-1, n_pkts, n_features, 1)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_cnn, ohe_y_train,\n",
    "        validation_data=(X_valid_cnn, ohe_y_valid),\n",
    "        epochs=epoche,\n",
    "        batch_size=50,\n",
    "        callbacks=[earlystop],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    pd.DataFrame(history.history).to_csv(f\"{output_dir}/training_history.csv\", index=False)\n",
    "\n",
    "    y_pred_probs = model.predict(X_test_cnn)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    df_soft = pd.DataFrame({\n",
    "        \"Actual\": y_test,\n",
    "        \"soft_values\": [\",\".join(map(str, p)) for p in y_pred_probs]\n",
    "    })\n",
    "    df_soft.to_csv(f\"{output_dir}/soft_values.dat\", sep=\"\\t\", index=False)\n",
    "\n",
    "    df_pred = pd.DataFrame({\n",
    "        \"Actual\": y_test,\n",
    "        \"Predicted\": y_pred\n",
    "    })\n",
    "    df_pred.to_csv(f\"{output_dir}/predictions.dat\", sep=\"\\t\", index=False)\n",
    "\n",
    "    labels_map = {str(enc): cls for cls, enc in zip(le.classes_, le.transform(le.classes_))}\n",
    "    with open(f\"{output_dir}/labels_map.json\", \"w\") as f:\n",
    "        json.dump(labels_map, f)\n",
    "\n",
    "    print(\"✓ Training completato e file salvati.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5e0ca",
   "metadata": {},
   "source": [
    "# Addestramento di benchmarking su dataset ridimensinato a 45k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f073eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "720/720 - 4s - loss: 2.2789 - accuracy: 0.4031 - val_loss: 2.1084 - val_accuracy: 0.4303 - 4s/epoch - 5ms/step\n",
      "Epoch 2/30\n",
      "720/720 - 3s - loss: 1.7300 - accuracy: 0.5313 - val_loss: 1.7370 - val_accuracy: 0.5367 - 3s/epoch - 4ms/step\n",
      "Epoch 3/30\n",
      "720/720 - 3s - loss: 1.5229 - accuracy: 0.5788 - val_loss: 1.5818 - val_accuracy: 0.5612 - 3s/epoch - 4ms/step\n",
      "Epoch 4/30\n",
      "720/720 - 3s - loss: 1.3944 - accuracy: 0.6057 - val_loss: 1.5995 - val_accuracy: 0.5502 - 3s/epoch - 4ms/step\n",
      "Epoch 5/30\n",
      "720/720 - 3s - loss: 1.2977 - accuracy: 0.6318 - val_loss: 1.4541 - val_accuracy: 0.5968 - 3s/epoch - 4ms/step\n",
      "Epoch 6/30\n",
      "720/720 - 3s - loss: 1.2251 - accuracy: 0.6472 - val_loss: 1.4259 - val_accuracy: 0.6089 - 3s/epoch - 4ms/step\n",
      "Epoch 7/30\n",
      "720/720 - 3s - loss: 1.1698 - accuracy: 0.6603 - val_loss: 1.4629 - val_accuracy: 0.6138 - 3s/epoch - 4ms/step\n",
      "Epoch 8/30\n",
      "720/720 - 3s - loss: 1.1177 - accuracy: 0.6729 - val_loss: 1.3460 - val_accuracy: 0.6341 - 3s/epoch - 4ms/step\n",
      "Epoch 9/30\n",
      "720/720 - 3s - loss: 1.0644 - accuracy: 0.6838 - val_loss: 1.3587 - val_accuracy: 0.6323 - 3s/epoch - 4ms/step\n",
      "Epoch 10/30\n",
      "720/720 - 3s - loss: 1.0314 - accuracy: 0.6940 - val_loss: 1.3775 - val_accuracy: 0.6260 - 3s/epoch - 4ms/step\n",
      "Epoch 11/30\n",
      "720/720 - 3s - loss: 0.9949 - accuracy: 0.7021 - val_loss: 1.3912 - val_accuracy: 0.6270 - 3s/epoch - 4ms/step\n",
      "Epoch 12/30\n",
      "720/720 - 3s - loss: 0.9638 - accuracy: 0.7104 - val_loss: 1.2889 - val_accuracy: 0.6563 - 3s/epoch - 4ms/step\n",
      "Epoch 13/30\n",
      "720/720 - 3s - loss: 0.9376 - accuracy: 0.7175 - val_loss: 1.2700 - val_accuracy: 0.6647 - 3s/epoch - 4ms/step\n",
      "Epoch 14/30\n",
      "720/720 - 3s - loss: 0.9142 - accuracy: 0.7234 - val_loss: 1.2625 - val_accuracy: 0.6698 - 3s/epoch - 4ms/step\n",
      "Epoch 15/30\n",
      "720/720 - 3s - loss: 0.8870 - accuracy: 0.7310 - val_loss: 1.4362 - val_accuracy: 0.6280 - 3s/epoch - 4ms/step\n",
      "Epoch 16/30\n",
      "720/720 - 3s - loss: 0.8617 - accuracy: 0.7354 - val_loss: 1.5630 - val_accuracy: 0.5951 - 3s/epoch - 4ms/step\n",
      "Epoch 17/30\n",
      "720/720 - 3s - loss: 0.8445 - accuracy: 0.7406 - val_loss: 1.2663 - val_accuracy: 0.6814 - 3s/epoch - 4ms/step\n",
      "Epoch 18/30\n",
      "720/720 - 3s - loss: 0.8282 - accuracy: 0.7415 - val_loss: 1.2692 - val_accuracy: 0.6776 - 3s/epoch - 4ms/step\n",
      "Epoch 19/30\n",
      "720/720 - 3s - loss: 0.8136 - accuracy: 0.7482 - val_loss: 1.3666 - val_accuracy: 0.6589 - 3s/epoch - 4ms/step\n",
      "Epoch 20/30\n",
      "720/720 - 3s - loss: 0.7950 - accuracy: 0.7534 - val_loss: 1.4017 - val_accuracy: 0.6439 - 3s/epoch - 4ms/step\n",
      "Epoch 21/30\n",
      "720/720 - 3s - loss: 0.7792 - accuracy: 0.7569 - val_loss: 1.2876 - val_accuracy: 0.6778 - 3s/epoch - 4ms/step\n",
      "Epoch 22/30\n",
      "720/720 - 3s - loss: 0.7598 - accuracy: 0.7640 - val_loss: 1.3461 - val_accuracy: 0.6674 - 3s/epoch - 4ms/step\n",
      "Epoch 23/30\n",
      "720/720 - 3s - loss: 0.7532 - accuracy: 0.7625 - val_loss: 1.3109 - val_accuracy: 0.6771 - 3s/epoch - 4ms/step\n",
      "Epoch 24/30\n",
      "720/720 - 3s - loss: 0.7442 - accuracy: 0.7682 - val_loss: 1.3598 - val_accuracy: 0.6676 - 3s/epoch - 4ms/step\n",
      "Epoch 25/30\n",
      "720/720 - 3s - loss: 0.7256 - accuracy: 0.7742 - val_loss: 1.3322 - val_accuracy: 0.6794 - 3s/epoch - 4ms/step\n",
      "Epoch 26/30\n",
      "720/720 - 3s - loss: 0.7198 - accuracy: 0.7742 - val_loss: 1.2566 - val_accuracy: 0.6990 - 3s/epoch - 4ms/step\n",
      "Epoch 27/30\n",
      "720/720 - 3s - loss: 0.7177 - accuracy: 0.7734 - val_loss: 1.3924 - val_accuracy: 0.6582 - 3s/epoch - 4ms/step\n",
      "Epoch 28/30\n",
      "720/720 - 3s - loss: 0.6980 - accuracy: 0.7810 - val_loss: 1.3196 - val_accuracy: 0.6869 - 3s/epoch - 4ms/step\n",
      "Epoch 29/30\n",
      "720/720 - 3s - loss: 0.6956 - accuracy: 0.7782 - val_loss: 1.3131 - val_accuracy: 0.6853 - 3s/epoch - 4ms/step\n",
      "Epoch 30/30\n",
      "720/720 - 3s - loss: 0.6791 - accuracy: 0.7845 - val_loss: 1.3036 - val_accuracy: 0.6928 - 3s/epoch - 4ms/step\n",
      "607/607 [==============================] - 1s 2ms/step\n",
      "✓ Training completato e file salvati.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def ingest_dataset(path, n_pkts=10, n_features=4):\n",
    "    \"\"\"Carica dataset da pickle e seleziona i primi n_pkts e n_features.\"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        biflows = pickle.load(f)\n",
    "        labels = pickle.load(f)\n",
    "    biflows = np.array(biflows)[:, :n_pkts, :n_features]\n",
    "    return biflows, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataset_path = \"mirage2019_LOPEZ_lopez_lopez_36P_4F_APP_xST_PAD_metadata.pickle\"\n",
    "    output_dir = \"output4/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    n_pkts = 10\n",
    "    n_features = 4\n",
    "    sample_train_size = 45000\n",
    "    epoche = epoche\n",
    "    seed = 2025\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    X, y = ingest_dataset(dataset_path, n_pkts=n_pkts, n_features=n_features)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    num_classes = len(np.unique(y))\n",
    "\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=seed\n",
    "    )\n",
    "\n",
    "    if sample_train_size < len(X_train_full):\n",
    "        X_train_full, _, y_train_full, _ = train_test_split(\n",
    "            X_train_full, y_train_full,\n",
    "            train_size=sample_train_size,\n",
    "            stratify=y_train_full,\n",
    "            random_state=seed\n",
    "        )\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train_full_scaled = scaler.fit_transform(X_train_full.reshape(-1, n_features)).reshape(-1, n_pkts, n_features)\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, n_features)).reshape(-1, n_pkts, n_features)\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_train_full_scaled, y_train_full,\n",
    "        test_size=0.2, stratify=y_train_full, random_state=seed\n",
    "    )\n",
    "\n",
    "    ohe_y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "    ohe_y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=num_classes)\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (4, 2), padding='same', activation='relu', input_shape=(n_pkts, n_features, 1)),\n",
    "        MaxPooling2D((3, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (4, 2), padding='same', activation='relu'),\n",
    "        MaxPooling2D((3, 1), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    earlystop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "    with open(f\"{output_dir}/model_summary.txt\", \"w\") as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "    X_train_cnn = X_train.reshape(-1, n_pkts, n_features, 1)\n",
    "    X_valid_cnn = X_valid.reshape(-1, n_pkts, n_features, 1)\n",
    "    X_test_cnn  = X_test_scaled.reshape(-1, n_pkts, n_features, 1)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_cnn, ohe_y_train,\n",
    "        validation_data=(X_valid_cnn, ohe_y_valid),\n",
    "        epochs=epoche,\n",
    "        batch_size=50,\n",
    "        callbacks=[earlystop],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    pd.DataFrame(history.history).to_csv(f\"{output_dir}/training_history.csv\", index=False)\n",
    "\n",
    "    y_pred_probs = model.predict(X_test_cnn)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    df_soft = pd.DataFrame({\n",
    "        \"Actual\": y_test,\n",
    "        \"soft_values\": [\",\".join(map(str, p)) for p in y_pred_probs]\n",
    "    })\n",
    "    df_soft.to_csv(f\"{output_dir}/soft_values.dat\", sep=\"\\t\", index=False)\n",
    "\n",
    "    df_pred = pd.DataFrame({\n",
    "        \"Actual\": y_test,\n",
    "        \"Predicted\": y_pred\n",
    "    })\n",
    "    df_pred.to_csv(f\"{output_dir}/predictions.dat\", sep=\"\\t\", index=False)\n",
    "\n",
    "    labels_map = {str(enc): cls for cls, enc in zip(le.classes_, le.transform(le.classes_))}\n",
    "    with open(f\"{output_dir}/labels_map.json\", \"w\") as f:\n",
    "        json.dump(labels_map, f)\n",
    "\n",
    "    print(\"✓ Training completato e file salvati.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2422e2d",
   "metadata": {},
   "source": [
    "# Addestramento di benchmarking su dataset ridimensinato a 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc5bddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "800/800 - 4s - loss: 2.2305 - accuracy: 0.4160 - val_loss: 1.8889 - val_accuracy: 0.5048 - 4s/epoch - 5ms/step\n",
      "Epoch 2/30\n",
      "800/800 - 3s - loss: 1.6894 - accuracy: 0.5394 - val_loss: 1.6331 - val_accuracy: 0.5480 - 3s/epoch - 4ms/step\n",
      "Epoch 3/30\n",
      "800/800 - 3s - loss: 1.4919 - accuracy: 0.5851 - val_loss: 1.5085 - val_accuracy: 0.5871 - 3s/epoch - 3ms/step\n",
      "Epoch 4/30\n",
      "800/800 - 3s - loss: 1.3676 - accuracy: 0.6144 - val_loss: 1.4332 - val_accuracy: 0.6071 - 3s/epoch - 3ms/step\n",
      "Epoch 5/30\n",
      "800/800 - 3s - loss: 1.2754 - accuracy: 0.6352 - val_loss: 1.3846 - val_accuracy: 0.6217 - 3s/epoch - 3ms/step\n",
      "Epoch 6/30\n",
      "800/800 - 3s - loss: 1.2017 - accuracy: 0.6558 - val_loss: 1.4595 - val_accuracy: 0.5942 - 3s/epoch - 3ms/step\n",
      "Epoch 7/30\n",
      "800/800 - 3s - loss: 1.1454 - accuracy: 0.6680 - val_loss: 1.2788 - val_accuracy: 0.6468 - 3s/epoch - 3ms/step\n",
      "Epoch 8/30\n",
      "800/800 - 3s - loss: 1.0895 - accuracy: 0.6816 - val_loss: 1.4724 - val_accuracy: 0.5981 - 3s/epoch - 3ms/step\n",
      "Epoch 9/30\n",
      "800/800 - 3s - loss: 1.0500 - accuracy: 0.6921 - val_loss: 1.3851 - val_accuracy: 0.6256 - 3s/epoch - 3ms/step\n",
      "Epoch 10/30\n",
      "800/800 - 3s - loss: 1.0108 - accuracy: 0.6994 - val_loss: 1.4457 - val_accuracy: 0.6093 - 3s/epoch - 3ms/step\n",
      "Epoch 11/30\n",
      "800/800 - 3s - loss: 0.9755 - accuracy: 0.7081 - val_loss: 1.3380 - val_accuracy: 0.6398 - 3s/epoch - 3ms/step\n",
      "Epoch 12/30\n",
      "800/800 - 3s - loss: 0.9461 - accuracy: 0.7153 - val_loss: 1.3796 - val_accuracy: 0.6193 - 3s/epoch - 3ms/step\n",
      "Epoch 13/30\n",
      "800/800 - 3s - loss: 0.9157 - accuracy: 0.7246 - val_loss: 1.2428 - val_accuracy: 0.6558 - 3s/epoch - 3ms/step\n",
      "Epoch 14/30\n",
      "800/800 - 3s - loss: 0.8922 - accuracy: 0.7294 - val_loss: 1.2427 - val_accuracy: 0.6603 - 3s/epoch - 3ms/step\n",
      "Epoch 15/30\n",
      "800/800 - 3s - loss: 0.8703 - accuracy: 0.7355 - val_loss: 1.1550 - val_accuracy: 0.6937 - 3s/epoch - 3ms/step\n",
      "Epoch 16/30\n",
      "800/800 - 3s - loss: 0.8460 - accuracy: 0.7418 - val_loss: 1.2409 - val_accuracy: 0.6743 - 3s/epoch - 3ms/step\n",
      "Epoch 17/30\n",
      "800/800 - 3s - loss: 0.8321 - accuracy: 0.7451 - val_loss: 1.4019 - val_accuracy: 0.6322 - 3s/epoch - 3ms/step\n",
      "Epoch 18/30\n",
      "800/800 - 3s - loss: 0.8111 - accuracy: 0.7513 - val_loss: 1.2133 - val_accuracy: 0.6786 - 3s/epoch - 3ms/step\n",
      "Epoch 19/30\n",
      "800/800 - 3s - loss: 0.7934 - accuracy: 0.7559 - val_loss: 1.1996 - val_accuracy: 0.6892 - 3s/epoch - 4ms/step\n",
      "Epoch 20/30\n",
      "800/800 - 3s - loss: 0.7821 - accuracy: 0.7590 - val_loss: 1.1903 - val_accuracy: 0.6853 - 3s/epoch - 4ms/step\n",
      "Epoch 21/30\n",
      "800/800 - 3s - loss: 0.7663 - accuracy: 0.7615 - val_loss: 1.1569 - val_accuracy: 0.6925 - 3s/epoch - 4ms/step\n",
      "Epoch 22/30\n",
      "800/800 - 3s - loss: 0.7556 - accuracy: 0.7660 - val_loss: 1.1588 - val_accuracy: 0.7006 - 3s/epoch - 4ms/step\n",
      "Epoch 23/30\n",
      "800/800 - 3s - loss: 0.7384 - accuracy: 0.7704 - val_loss: 1.2608 - val_accuracy: 0.6839 - 3s/epoch - 4ms/step\n",
      "Epoch 24/30\n",
      "800/800 - 3s - loss: 0.7308 - accuracy: 0.7716 - val_loss: 1.3120 - val_accuracy: 0.6662 - 3s/epoch - 4ms/step\n",
      "Epoch 25/30\n",
      "800/800 - 3s - loss: 0.7149 - accuracy: 0.7769 - val_loss: 1.3064 - val_accuracy: 0.6685 - 3s/epoch - 4ms/step\n",
      "Epoch 26/30\n",
      "800/800 - 3s - loss: 0.7075 - accuracy: 0.7789 - val_loss: 1.1825 - val_accuracy: 0.7007 - 3s/epoch - 4ms/step\n",
      "Epoch 27/30\n",
      "800/800 - 3s - loss: 0.6960 - accuracy: 0.7808 - val_loss: 1.1516 - val_accuracy: 0.6998 - 3s/epoch - 4ms/step\n",
      "Epoch 28/30\n",
      "800/800 - 3s - loss: 0.6921 - accuracy: 0.7836 - val_loss: 1.1885 - val_accuracy: 0.6973 - 3s/epoch - 4ms/step\n",
      "Epoch 29/30\n",
      "800/800 - 3s - loss: 0.6813 - accuracy: 0.7843 - val_loss: 1.2191 - val_accuracy: 0.6961 - 3s/epoch - 4ms/step\n",
      "Epoch 30/30\n",
      "800/800 - 3s - loss: 0.6739 - accuracy: 0.7875 - val_loss: 1.1921 - val_accuracy: 0.7049 - 3s/epoch - 4ms/step\n",
      "607/607 [==============================] - 1s 2ms/step\n",
      "✓ Training completato e file salvati.\n",
      "Epoch 1/30\n",
      "800/800 - 4s - loss: 2.3225 - accuracy: 0.3942 - val_loss: 1.9520 - val_accuracy: 0.4782 - 4s/epoch - 5ms/step\n",
      "Epoch 2/30\n",
      "800/800 - 3s - loss: 1.7589 - accuracy: 0.5203 - val_loss: 1.7431 - val_accuracy: 0.5333 - 3s/epoch - 4ms/step\n",
      "Epoch 3/30\n",
      "800/800 - 3s - loss: 1.5466 - accuracy: 0.5709 - val_loss: 1.5722 - val_accuracy: 0.5717 - 3s/epoch - 4ms/step\n",
      "Epoch 4/30\n",
      "800/800 - 3s - loss: 1.4168 - accuracy: 0.6011 - val_loss: 1.4946 - val_accuracy: 0.5927 - 3s/epoch - 3ms/step\n",
      "Epoch 5/30\n",
      "800/800 - 3s - loss: 1.3155 - accuracy: 0.6253 - val_loss: 1.3694 - val_accuracy: 0.6219 - 3s/epoch - 3ms/step\n",
      "Epoch 6/30\n",
      "800/800 - 3s - loss: 1.2363 - accuracy: 0.6437 - val_loss: 1.4288 - val_accuracy: 0.6040 - 3s/epoch - 3ms/step\n",
      "Epoch 7/30\n",
      "800/800 - 3s - loss: 1.1783 - accuracy: 0.6570 - val_loss: 1.3735 - val_accuracy: 0.6301 - 3s/epoch - 3ms/step\n",
      "Epoch 8/30\n",
      "800/800 - 3s - loss: 1.1253 - accuracy: 0.6714 - val_loss: 1.3256 - val_accuracy: 0.6396 - 3s/epoch - 3ms/step\n",
      "Epoch 9/30\n",
      "800/800 - 3s - loss: 1.0861 - accuracy: 0.6791 - val_loss: 1.2603 - val_accuracy: 0.6645 - 3s/epoch - 3ms/step\n",
      "Epoch 10/30\n",
      "800/800 - 3s - loss: 1.0433 - accuracy: 0.6920 - val_loss: 1.2836 - val_accuracy: 0.6588 - 3s/epoch - 3ms/step\n",
      "Epoch 11/30\n",
      "800/800 - 3s - loss: 1.0090 - accuracy: 0.7000 - val_loss: 1.4136 - val_accuracy: 0.6225 - 3s/epoch - 3ms/step\n",
      "Epoch 12/30\n",
      "800/800 - 3s - loss: 0.9839 - accuracy: 0.7049 - val_loss: 1.5029 - val_accuracy: 0.6177 - 3s/epoch - 3ms/step\n",
      "Epoch 13/30\n",
      "800/800 - 3s - loss: 0.9546 - accuracy: 0.7132 - val_loss: 1.2912 - val_accuracy: 0.6608 - 3s/epoch - 3ms/step\n",
      "Epoch 14/30\n",
      "800/800 - 3s - loss: 0.9259 - accuracy: 0.7210 - val_loss: 1.2339 - val_accuracy: 0.6821 - 3s/epoch - 3ms/step\n",
      "Epoch 15/30\n",
      "800/800 - 3s - loss: 0.9058 - accuracy: 0.7246 - val_loss: 1.3012 - val_accuracy: 0.6573 - 3s/epoch - 4ms/step\n",
      "Epoch 16/30\n",
      "800/800 - 3s - loss: 0.8848 - accuracy: 0.7296 - val_loss: 1.1894 - val_accuracy: 0.6875 - 3s/epoch - 4ms/step\n",
      "Epoch 17/30\n",
      "800/800 - 3s - loss: 0.8671 - accuracy: 0.7335 - val_loss: 1.7114 - val_accuracy: 0.5551 - 3s/epoch - 4ms/step\n",
      "Epoch 18/30\n",
      "800/800 - 3s - loss: 0.8471 - accuracy: 0.7411 - val_loss: 1.5017 - val_accuracy: 0.6210 - 3s/epoch - 4ms/step\n",
      "Epoch 19/30\n",
      "800/800 - 3s - loss: 0.8328 - accuracy: 0.7450 - val_loss: 1.3514 - val_accuracy: 0.6486 - 3s/epoch - 4ms/step\n",
      "Epoch 20/30\n",
      "800/800 - 3s - loss: 0.8203 - accuracy: 0.7462 - val_loss: 1.2520 - val_accuracy: 0.6846 - 3s/epoch - 4ms/step\n",
      "Epoch 21/30\n",
      "800/800 - 3s - loss: 0.8063 - accuracy: 0.7488 - val_loss: 1.2739 - val_accuracy: 0.6753 - 3s/epoch - 4ms/step\n",
      "Epoch 22/30\n",
      "800/800 - 3s - loss: 0.7890 - accuracy: 0.7543 - val_loss: 1.4510 - val_accuracy: 0.6341 - 3s/epoch - 4ms/step\n",
      "Epoch 23/30\n",
      "800/800 - 3s - loss: 0.7735 - accuracy: 0.7601 - val_loss: 1.2445 - val_accuracy: 0.6923 - 3s/epoch - 4ms/step\n",
      "Epoch 24/30\n",
      "800/800 - 3s - loss: 0.7709 - accuracy: 0.7576 - val_loss: 1.1742 - val_accuracy: 0.7004 - 3s/epoch - 4ms/step\n",
      "Epoch 25/30\n",
      "800/800 - 3s - loss: 0.7530 - accuracy: 0.7644 - val_loss: 1.6655 - val_accuracy: 0.5931 - 3s/epoch - 4ms/step\n",
      "Epoch 26/30\n",
      "800/800 - 3s - loss: 0.7460 - accuracy: 0.7667 - val_loss: 1.3957 - val_accuracy: 0.6519 - 3s/epoch - 4ms/step\n",
      "Epoch 27/30\n",
      "800/800 - 3s - loss: 0.7324 - accuracy: 0.7685 - val_loss: 2.7000 - val_accuracy: 0.4797 - 3s/epoch - 4ms/step\n",
      "Epoch 28/30\n",
      "800/800 - 3s - loss: 0.7280 - accuracy: 0.7707 - val_loss: 1.2989 - val_accuracy: 0.6805 - 3s/epoch - 4ms/step\n",
      "Epoch 29/30\n",
      "800/800 - 3s - loss: 0.7179 - accuracy: 0.7730 - val_loss: 1.2084 - val_accuracy: 0.7084 - 3s/epoch - 4ms/step\n",
      "Epoch 30/30\n",
      "800/800 - 3s - loss: 0.7074 - accuracy: 0.7767 - val_loss: 2.2643 - val_accuracy: 0.5357 - 3s/epoch - 4ms/step\n",
      "607/607 [==============================] - 1s 2ms/step\n",
      "✓ Training completato e file salvati.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "for i in range(0,2):\n",
    "    seed = 2025 + i\n",
    "    def ingest_dataset(path, n_pkts=10, n_features=4):\n",
    "        \"\"\"Carica dataset da pickle e seleziona i primi n_pkts e n_features.\"\"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            biflows = pickle.load(f)\n",
    "            labels = pickle.load(f)\n",
    "        biflows = np.array(biflows)[:, :n_pkts, :n_features]\n",
    "        return biflows, labels\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "\n",
    "        dataset_path = \"mirage2019_LOPEZ_lopez_lopez_36P_4F_APP_xST_PAD_metadata.pickle\"\n",
    "        output_dir = \"output5/\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        n_pkts = 10\n",
    "        n_features = 4\n",
    "        sample_train_size = 50000\n",
    "        epoche = epoche\n",
    "        seed = seed\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "        X, y = ingest_dataset(dataset_path, n_pkts=n_pkts, n_features=n_features)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        num_classes = len(np.unique(y))\n",
    "\n",
    "        X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=seed\n",
    "        )\n",
    "\n",
    "        if sample_train_size < len(X_train_full):\n",
    "            X_train_full, _, y_train_full, _ = train_test_split(\n",
    "                X_train_full, y_train_full,\n",
    "                train_size=sample_train_size,\n",
    "                stratify=y_train_full,\n",
    "                random_state=seed\n",
    "            )\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_train_full_scaled = scaler.fit_transform(X_train_full.reshape(-1, n_features)).reshape(-1, n_pkts, n_features)\n",
    "        X_test_scaled = scaler.transform(X_test.reshape(-1, n_features)).reshape(-1, n_pkts, n_features)\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_train_full_scaled, y_train_full,\n",
    "            test_size=0.2, stratify=y_train_full, random_state=seed\n",
    "        )\n",
    "\n",
    "        ohe_y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "        ohe_y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=num_classes)\n",
    "\n",
    "        model = Sequential([\n",
    "            Conv2D(32, (4, 2), padding='same', activation='relu', input_shape=(n_pkts, n_features, 1)),\n",
    "            MaxPooling2D((3, 2), padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(64, (4, 2), padding='same', activation='relu'),\n",
    "            MaxPooling2D((3, 1), padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        earlystop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "        with open(f\"{output_dir}/model_summary.txt\", \"w\") as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "        X_train_cnn = X_train.reshape(-1, n_pkts, n_features, 1)\n",
    "        X_valid_cnn = X_valid.reshape(-1, n_pkts, n_features, 1)\n",
    "        X_test_cnn  = X_test_scaled.reshape(-1, n_pkts, n_features, 1)\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train_cnn, ohe_y_train,\n",
    "            validation_data=(X_valid_cnn, ohe_y_valid),\n",
    "            epochs=epoche,\n",
    "            batch_size=50,\n",
    "            callbacks=[earlystop],\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        pd.DataFrame(history.history).to_csv(f\"{output_dir}/training_history{i}.csv\", index=False)\n",
    "\n",
    "        y_pred_probs = model.predict(X_test_cnn)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "        df_soft = pd.DataFrame({\n",
    "            \"Actual\": y_test,\n",
    "            \"soft_values\": [\",\".join(map(str, p)) for p in y_pred_probs]\n",
    "        })\n",
    "        df_soft.to_csv(f\"{output_dir}/soft_values{i}.dat\", sep=\"\\t\", index=False)\n",
    "\n",
    "        df_pred = pd.DataFrame({\n",
    "            \"Actual\": y_test,\n",
    "            \"Predicted\": y_pred\n",
    "        })\n",
    "        df_pred.to_csv(f\"{output_dir}/predictions{i}.dat\", sep=\"\\t\", index=False)\n",
    "\n",
    "        labels_map = {str(enc): cls for cls, enc in zip(le.classes_, le.transform(le.classes_))}\n",
    "        with open(f\"{output_dir}/labels_map{i}.json\", \"w\") as f:\n",
    "            json.dump(labels_map, f)\n",
    "\n",
    "        print(\"✓ Training completato e file salvati.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
